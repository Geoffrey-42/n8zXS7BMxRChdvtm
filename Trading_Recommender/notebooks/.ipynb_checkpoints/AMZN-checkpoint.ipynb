{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b7fcbdd-668b-4c64-b4a5-0f6643dc1825",
   "metadata": {},
   "source": [
    "# __Amazon stock price forecasting (AMZN)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81750d26-9224-43ab-bef8-60d9346b7e5b",
   "metadata": {},
   "source": [
    "Set working directory to Trading_Recommender folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fd235d5-04b4-42ab-a3ba-d34b692d486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a5b5d-7153-4759-899c-1ee431c0c426",
   "metadata": {},
   "source": [
    "import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36223f66-4195-4e78-a19b-7f7862011ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking __init__.py for src\n",
      "Invoking __init__.py for src.data\n",
      "Invoking __init__.py for src.models\n",
      "Invoking __init__.py for src.models.LSTM_files\n",
      "Invoking __init__.py for src.forecast\n",
      "Invoking __init__.py for src.tuning\n",
      "Invoking __init__.py for src.features\n"
     ]
    }
   ],
   "source": [
    "from src.data.extract_dataset import extract_financial_data\n",
    "from src.features.extend_dataset import compute_technical_indicators, merge_dataframes, get_target\n",
    "from src.features.feature_selection import select_features, evaluate_features\n",
    "from src.features.feature_engineering import engineer_features, scale_dataframe, get_final_dataframe\n",
    "from src.forecast.forecaster import forecaster\n",
    "from src.forecast.recommender import recommender\n",
    "from src.tuning.optuna_tuning import optuna_search\n",
    "import optuna_dashboard\n",
    "from optuna.storages import JournalStorage, JournalFileStorage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from itertools import accumulate\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2d95f3-bc9e-43f6-a794-25195c2b3328",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = 'AMZN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051740e-ee46-4cbd-a6a6-88f1483ab7e3",
   "metadata": {},
   "source": [
    "Extract the data from locally saved files. There is also an option to download stock market data from yfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406ca82-a755-482e-9d91-7e4cdb4d7a7e",
   "metadata": {},
   "source": [
    "The raw data that will be used can be classified into three categories:  \n",
    "1) Stock market data (Open, Close, High, Low, Volume)  \n",
    "2) SMIS macro-economic indicators (^IXIC, ^GSPC, DJI)  \n",
    "3) Internet trend data (googletrends, wikipediatrends)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e95bd58-860d-4015-a232-c55ba733ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dict, smis_dict, trend_dict = extract_financial_data(data_dir = 'data', \n",
    "                                                           save=False, online=False)\n",
    "stock = stock_dict[stock_name]\n",
    "close = stock[['Close']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9abfa-35a4-432b-b71d-8eb98287eeab",
   "metadata": {},
   "source": [
    "Extend the raw features by computing a large number technical indicators using pandas-ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0d96c0-9121-4130-a576-e9dba0898404",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_extended, smis_extended, trend_extended = compute_technical_indicators(stock,\n",
    "                                                                             deepcopy(smis_dict),\n",
    "                                                                             deepcopy(trend_dict[stock_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e612e-075d-4138-8cb2-0e88b4430aba",
   "metadata": {},
   "source": [
    "These various types of data will now be merged into only one pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095a4d81-ba91-4262-a0a2-5bb196292d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df_withna = merge_dataframes(stock_extended, smis_extended, trend_extended)\n",
    "extended_df = extended_df_withna.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b7040-6985-475a-a6a3-981735030661",
   "metadata": {},
   "source": [
    "Check the number of technical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec0eee2b-14a6-4aa8-ac72-6dd0bb67082f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 260\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of features: {extended_df.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27be45a6-9d54-458c-b195-34654b5cbc67",
   "metadata": {},
   "source": [
    "Check the number of rows and the date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48a6122-05f7-4bc7-9d2b-c0b6aff013fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days: 1469\n",
      "Date Range: from 2020-04-23 00:00:00  to  2024-04-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of days: {extended_df.shape[0]}\\nDate Range: from {extended_df.index[0]}  to  {extended_df.index[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca305cc6-25ba-4ec1-a588-72b150d9c77c",
   "metadata": {},
   "source": [
    "Next, a feature selection will be performed using sktime.  \n",
    "This will be based on whether the selected features are informative enough to predict the binary labels y.  \n",
    "The labels indicate for each row whether the stock price will be increasing or decreasing {horizon} days into the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b207a4ac-9f00-4349-b3ac-c7f5033ce97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 14 # How many days ahead do we want to forecast the stock price evolution\n",
    "target = get_target(extended_df,\n",
    "                    horizon) # This is the return defined as Close[i+horizon]/Close[i] - 1\n",
    "y = (target>0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd86e15-2afa-4847-a212-a4a70d717a9c",
   "metadata": {},
   "source": [
    "In the next cell, a much smaller number of relevant features are selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "733cd82e-e9c6-44a6-b5ea-9267cba54c24",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m regressor \u001b[38;5;241m=\u001b[39m GradientBoostingRegressor(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m features_selected, feature_importances \u001b[38;5;241m=\u001b[39m select_features(extended_df,\n\u001b[0;32m      3\u001b[0m                                                          horizon \u001b[38;5;241m=\u001b[39m horizon,\n\u001b[0;32m      4\u001b[0m                                                          n_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m                                                          regressor \u001b[38;5;241m=\u001b[39m regressor,\n\u001b[0;32m      6\u001b[0m                                                          importance_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m)\n\u001b[0;32m      7\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(features_selected\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Printing information about the selected features\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\Apziva\\ValueInvestor\\n8zXS7BMxRChdvtm\\Trading_Recommender\\src\\features\\feature_selection.py:64\u001b[0m, in \u001b[0;36mselect_features\u001b[1;34m(extended_df, horizon, n_columns, regressor, importance_threshold)\u001b[0m\n\u001b[0;32m     59\u001b[0m X, y \u001b[38;5;241m=\u001b[39m features_scaled[:train_index, :], y[:train_index]\n\u001b[0;32m     61\u001b[0m selector \u001b[38;5;241m=\u001b[39m FeatureSelection(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature-importances\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     62\u001b[0m                             regressor \u001b[38;5;241m=\u001b[39m regressor,\n\u001b[0;32m     63\u001b[0m                             n_columns \u001b[38;5;241m=\u001b[39m n_columns)\n\u001b[1;32m---> 64\u001b[0m selector\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m     66\u001b[0m feature_importances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([(extended_df\u001b[38;5;241m.\u001b[39mcolumns[i], selector\u001b[38;5;241m.\u001b[39mfeature_importances_[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m selector\u001b[38;5;241m.\u001b[39mcolumns_], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m item:item[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((i \u001b[38;5;28;01mfor\u001b[39;00m i, total \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(accumulate(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[38;5;241m1\u001b[39m], feature_importances))) \u001b[38;5;28;01mif\u001b[39;00m total \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m importance_threshold), \u001b[38;5;28mlen\u001b[39m(feature_importances)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sktime\\transformations\\base.py:498\u001b[0m, in \u001b[0;36mBaseTransformer.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# we call the ordinary _fit if no looping/vectorization needed\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vectorization_needed:\n\u001b[1;32m--> 498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X\u001b[38;5;241m=\u001b[39mX_inner, y\u001b[38;5;241m=\u001b[39my_inner)\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# otherwise we call the vectorized version of fit\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m=\u001b[39mX_inner, y\u001b[38;5;241m=\u001b[39my_inner)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sktime\\transformations\\series\\feature_selection.py:133\u001b[0m, in \u001b[0;36mFeatureSelection._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_columns(X)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# fit regressor with X as exog data and y as endog data (target)\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor_\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"The given regressor must have an\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m        attribute feature_importances_ after fitting.\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:538\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stages(\n\u001b[0;32m    539\u001b[0m     X,\n\u001b[0;32m    540\u001b[0m     y,\n\u001b[0;32m    541\u001b[0m     raw_predictions,\n\u001b[0;32m    542\u001b[0m     sample_weight,\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[0;32m    544\u001b[0m     X_val,\n\u001b[0;32m    545\u001b[0m     y_val,\n\u001b[0;32m    546\u001b[0m     sample_weight_val,\n\u001b[0;32m    547\u001b[0m     begin_at_stage,\n\u001b[0;32m    548\u001b[0m     monitor,\n\u001b[0;32m    549\u001b[0m )\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:615\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    608\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    609\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    610\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    611\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    612\u001b[0m     )\n\u001b[0;32m    614\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 615\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stage(\n\u001b[0;32m    616\u001b[0m     i,\n\u001b[0;32m    617\u001b[0m     X,\n\u001b[0;32m    618\u001b[0m     y,\n\u001b[0;32m    619\u001b[0m     raw_predictions,\n\u001b[0;32m    620\u001b[0m     sample_weight,\n\u001b[0;32m    621\u001b[0m     sample_mask,\n\u001b[0;32m    622\u001b[0m     random_state,\n\u001b[0;32m    623\u001b[0m     X_csc,\n\u001b[0;32m    624\u001b[0m     X_csr,\n\u001b[0;32m    625\u001b[0m )\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:257\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    254\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    256\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 257\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(X, residual, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    260\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    261\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    262\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    270\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m   1248\u001b[0m         X,\n\u001b[0;32m   1249\u001b[0m         y,\n\u001b[0;32m   1250\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1251\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m   1252\u001b[0m     )\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "regressor = GradientBoostingRegressor(max_depth=1)\n",
    "features_selected, feature_importances = select_features(extended_df,\n",
    "                                                         horizon = horizon,\n",
    "                                                         n_columns = None,\n",
    "                                                         regressor = regressor,\n",
    "                                                         importance_threshold = 0.99)\n",
    "feature_names = list(features_selected.columns)\n",
    "# Printing information about the selected features\n",
    "print(f'Using {regressor = }, only {features_selected.shape[1]} features are selected')\n",
    "print(f'The selected features with {horizon = } are: \\n{feature_names}')\n",
    "print(f'With a cumulated relative importance of {sum(map(lambda item: item[1], feature_importances))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff68450-1b08-48bc-ae15-5faa1ab8fcc8",
   "metadata": {},
   "source": [
    "The selected features predictive power are evaluated using the Rocket(num_kernels=2000) and HIVECOTEV2(time_limit_in_minutes=0.2) classifier instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc847c-a915-4aa6-82be-2c04eed8d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df, _ = get_final_dataframe(features_selected,\n",
    "                                     target) # Concatenates features_selected with target named as Return, and scales the dataframe\n",
    "untransformed_result = evaluate_features(selected_df.drop(columns=['Return']), y)\n",
    "untransformed_accuracies = (untransformed_result['HIVECOTEV2_accuracy'],\n",
    "                            untransformed_result['Rocket_accuracy'])\n",
    "print(f'The HIVECOTEV2 and Rocket classifiers respective accuracies using the {features_selected.shape[1]} untransformed features are: {untransformed_accuracies}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0e83c-6180-4c12-b017-a266691be3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in untransformed_result.items():\n",
    "    print(f'{key} = {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ed9af-028c-4e70-bc28-d925583e01e0",
   "metadata": {},
   "source": [
    "Next, the selected features are transformed using the Principal Component Analysis algorithm  \n",
    "The engineered features predictive power are evaluated using the Rocket(num_kernels=2000) and HIVECOTEV2(time_limit_in_minutes=0.2) classifier instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a95b4a-5175-4889-9494-e04298bbe940",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_threshold = 0.9\n",
    "features_engineered, transformer = engineer_features(features_selected, \n",
    "                                                     variance_threshold)\n",
    "engineered_df, scalers = get_final_dataframe(features_engineered,\n",
    "                                             target) # Concatenates features_engineered with target named as Return, and scales the dataframe\n",
    "data_scaler, target_scaler = scalers\n",
    "\n",
    "transformed_result = evaluate_features(engineered_df.drop(columns=['Return']), y)\n",
    "transformed_accuracies = (transformed_result['HIVECOTEV2_accuracy'],\n",
    "                          transformed_result['Rocket_accuracy'])\n",
    "print(f'The HIVECOTEV2 and Rocket classifiers respective accuracies using the {features_engineered.shape[1]} transformed features are: {transformed_accuracies}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5311e53-7006-405a-9e6e-17e59defd112",
   "metadata": {},
   "source": [
    "Despite the fact that the 10 engineered features only reflect about 92% of the variance of the original 24 selected features, the classifiers perform better with these engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c16395-1f77-4e75-a27d-cb4021a40d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in transformed_result.items():\n",
    "    print(f'{key} = {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32bee2d-5c22-4c4f-a54c-a0eb12b0c5ee",
   "metadata": {},
   "source": [
    "Next, some information on the engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba8fb5-fdae-409a-a809-70f892ae1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = transformer.get_covariance()\n",
    "explained_variance = transformer.explained_variance_/sum(transformer.explained_variance_)\n",
    "components = transformer.components_[:features_engineered.shape[1], :]\n",
    "contributions = np.abs(components)/np.sum(np.abs(components), axis = 1, keepdims=True)\n",
    "contributions_df = pd.DataFrame(contributions, columns=features_selected.columns, index=[f'PC {i+1} (Variance {explained_variance[i]:.4f})' for i in range(components.shape[0])])\n",
    "contributing_features = [sorted([(contributions_df.columns[i], value) for i, value in enumerate(contributions_df.iloc[j])], key = lambda x:x[1], reverse=True) for j in range(contributions_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272fd49-dc16-4528-8d98-a06025d8c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(10, 15))\n",
    "\n",
    "for i, component in enumerate(contributing_features[:2]):\n",
    "    features = [x[0] for x in component]\n",
    "    values = [x[1] for x in component]\n",
    "\n",
    "    axs[i].bar(features, values)\n",
    "    axs[i].tick_params(axis='x', rotation=90)\n",
    "    axs[i].tick_params(axis='x', labelsize=8)\n",
    "    axs[i].set_title(f'Principal Component {i+1} (Variance relative contribution = {explained_variance[i]:.4f})')\n",
    "    axs[i].set_xlabel('Features used')\n",
    "    axs[i].set_ylabel('Relative contribution of the feature')\n",
    "\n",
    "plt.tight_layout(pad=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d70c5-ed0e-4754-b8dd-bd352151e6f7",
   "metadata": {},
   "source": [
    "It can be noted that: \n",
    "- The two main Principal Components hold more than half of the total variance\n",
    "- Some SMIS macroeconomic indicators (^IXIC, ^GSPC) contribute significantly to the two main Principal Components\n",
    "- Statistics on searched keywords like Amazon and AmazonPrime on Google and Wikipedia also contribute to the two main Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b205c0-d6a4-4d73-8f3d-a8ec2668335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49af8cc-fcc3-409e-8d34-bd1997821d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6276d7a3-aaa7-4a77-8184-fb67a0a135dc",
   "metadata": {},
   "source": [
    "Now, the features are fully processed.\n",
    "Next, the hyperparameters of a LSTM model will be tuned using optuna to forecast the stock prices.  \n",
    "First, the arguments necessary for the study will be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e19d07-b5ee-4688-9a45-096b4804fc11",
   "metadata": {},
   "source": [
    "Data arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1531ad07-83d4-4c2d-aaec-825d6f3b7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = {'features': engineered_df,\n",
    "             'close': close,\n",
    "             'f_scaler': data_scaler,\n",
    "             't_scaler': target_scaler,\n",
    "             'symbol': stock_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19453184-558f-462f-b35a-0e8428b2a493",
   "metadata": {},
   "source": [
    "Temporal arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6667f-bfd1-4df1-99df-849227421515",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime('2023-03-01')\n",
    "end_date = pd.to_datetime('2023-12-31')\n",
    "temporal_args = {'start_date': start_date,\n",
    "                 'end_date': end_date,\n",
    "                 'horizon': horizon,\n",
    "                 'plot_start_date': pd.to_datetime('2023-01-01')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd7b59-70f5-40ec-af4d-381d147d146c",
   "metadata": {},
   "source": [
    "Model choice (LSTM) and default model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae0e6c-44bd-4527-91a1-1c96386935af",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_name = 'LSTM'\n",
    "model_args = {'seq_len': 30,\n",
    "              'learning_rate': 0.001,\n",
    "              'loss': 'mse',\n",
    "              'n_a': 16,\n",
    "              'dropout': 0.05,\n",
    "              'stateful_training': False,\n",
    "              'stateful_inference': False,\n",
    "              'horizon': horizon}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d27b8b-b073-42a3-96fe-d83679c68726",
   "metadata": {},
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46960e94-6f6b-48ad-b434-9289f72b11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {'epochs': 100,\n",
    "                 'batch_size': 32,\n",
    "                 'shuffle': False,\n",
    "                 'verbose': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450b96e-ff1d-48a9-89b7-63ba059859fe",
   "metadata": {},
   "source": [
    "Trading parameters (for the simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc2b91-64a6-4b85-ae9c-68ca8be8227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_stock = 1\n",
    "max_trade = 1\n",
    "intensity = 3 # Price variation by 1/intensity results in trading max_trade\n",
    "min_rate = 0.001 # Minimum daily rate of relative price change to trigger trading action\n",
    "trading_args = {'initial_stock': initial_stock,\n",
    "                'max_trade': max_trade,\n",
    "                'intensity': intensity,\n",
    "                'min_delta': min_rate*horizon}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecdea17-2583-4947-bac2-342a86ec7078",
   "metadata": {},
   "source": [
    "Creating an optuna study (hyperparameter search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83692a1-5713-45bb-820f-c61f369b8ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_search = True\n",
    "num_trials = 50 # If the study already exists, it will be continued with {num_trials} new trials\n",
    "if hyperparameter_search:\n",
    "    study_name = f'LSTM_ahead={horizon}'\n",
    "    storage_name = f'model_tuning_for_{stock_name}'\n",
    "    # Define the study storage method\n",
    "    storage = JournalStorage(JournalFileStorage(f\"src/tuning/{storage_name}.log\"))\n",
    "    # Pack the arguments\n",
    "    args = (model_args, data_args, temporal_args, training_args, trading_args)\n",
    "    # Launch the search\n",
    "    study = optuna_search(num_trials,\n",
    "                          storage,\n",
    "                          study_name,\n",
    "                          args,\n",
    "                          na_range = (1, 64),\n",
    "                          lr_range = (0.0001, 0.01),\n",
    "                          seq_len_range = (1, 120),\n",
    "                          dropout_range = (0, 0.2)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d748c0-0d0f-4ca9-b500-4631ccb96f6f",
   "metadata": {},
   "source": [
    "Information about the study's best trial and retrieval of best model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540f3f4-5da2-40e6-ae19-37e661b4e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best trial, its performance metric and its parameters\n",
    "best_trial = study.best_trial\n",
    "print(\"\\nNumber of finished trials: %s\"%len(study.trials))\n",
    "print(f\"\\nBest trial: {best_trial}\")\n",
    "print(\"  MSE: \", best_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Get the best parameters\n",
    "model_args['n_a'] = best_trial.params['n_a']\n",
    "model_args['learning_rate'] = best_trial.params['learning_rate']\n",
    "model_args['seq_len'] = best_trial.params['seq_len']\n",
    "model_args['dropout'] = best_trial.params['dropout']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a974c-122b-4d2a-a64d-2c70f17c6016",
   "metadata": {},
   "source": [
    "Run the optuna dashboard to visualize the study results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb20cb-2fa6-404d-ba52-9c0dbe906541",
   "metadata": {},
   "source": [
    "optuna_dashboard.run_server(storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41320c31-9e69-455f-9f22-f6dad82b6bdc",
   "metadata": {},
   "source": [
    "Test the tuned model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e3da7-c700-4081-8237-e703128f4a31",
   "metadata": {},
   "source": [
    "test = False\n",
    "if test:\n",
    "    ## Set the temporal parameters (Should define the test set)\n",
    "    start_date = pd.to_datetime('2024-01-01')\n",
    "    end_date = pd.to_datetime('2024-04-30')\n",
    "    temporal_args = {'start_date': start_date,\n",
    "                     'end_date': end_date,\n",
    "                     'horizon': horizon}\n",
    "    \n",
    "    ## Create a forecaster object\n",
    "    clairvoyant = forecaster(predictor_name,\n",
    "                             model_args)\n",
    "    \n",
    "    ## Create a recommender object\n",
    "    recommend = recommender(oracle = clairvoyant,\n",
    "                            trading_args = trading_args)\n",
    "    \n",
    "    \n",
    "    ### Simulate forecasting and recommendations\n",
    "    recommend(data_args,\n",
    "              temporal_args,\n",
    "              training_args) # performs the recommendation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
